%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: t
%%% End:

\documentclass[11pt]{scrartcl}
\usepackage[fancy, beaue, pset, anon]{sdll}
\pSet{\nt{MAT247}{I}{Introduction}}
\usepackage{lineno}
% ----------------------------------------------------------------------
% Page setup
% ----------------------------------------------------------------------

\pagenumbering{gobble}

% ----------------------------------------------------------------------
% Custom commands
% ----------------------------------------------------------------------

% alignment

\newcommand*{\LongestHence}{$\Rightarrow$}% function name
\newcommand*{\LongestName}{$f_o(-x)+f_e(-x)$}% function name
\newcommand*{\LongestValue}{$(-a)x +(-a)(-y)$}% function value
\newcommand*{\LongestText}{\defi}%

\newlength{\LargestHenceSize}%
\newlength{\LargestNameSize}%
\newlength{\LargestValueSize}%
\newlength{\LargestTextSize}%

\settowidth{\LargestHenceSize}{\LongestHence}%
\settowidth{\LargestNameSize}{\LongestName}%
\settowidth{\LargestValueSize}{\LongestValue}%
\settowidth{\LargestTextSize}{\LongestText}%

% Choose alignment of the various elements here: [r], [l] or [c]

\newcommand*{\mbh}[1]{{\makebox[\LargestHenceSize][r]{\ensuremath{#1}}}}%
\newcommand*{\mbn}[1]{{\makebox[\LargestNameSize][r]{\ensuremath{#1}}}}%
\newcommand*{\mbv}[1]{\ensuremath{\makebox[\LargestValueSize][r]{\ensuremath{#1}}}}%
\newcommand*{\mbt}[1]{\makebox[\LargestTextSize][l]{#1}}%

\newcommand{\R}[1]{\label{#1}\linelabel{#1}}
\newcommand{\lr}[1]{line~\lineref{#1}}

% ----------------------------------------------------------------------
% Launch!
% ----------------------------------------------------------------------
\begin{document}

Agenda: Chapter 5-7 in Friedberg et al

Marking Scheme: HW, Term Test (Thu, Feb 16), Final (13.3/26.7/60 OR
20/40/40)

Office Hours: Florian Herzig, Wednesday 3-4 pm (BA6186)
\section{Review of Determinants}
\label{sec:det}
Let \(F\) be a field.

Let \(A\in M_{n\times n}(F) \to \det(A) \in F \).

Note that for \(n=1\), \(\det(a) = a\).

If \(n=2\), \(\det \begin{pmatrix}
                     a & b\\
                     c & d
                   \end{pmatrix} = ad-bc
                   \)

In general, compute the determinant by expanding along a row/column.

For example, expansion along row:

\[
  \det A = \sum_{j=1} ^{n} (-1)^{i+j}A_{ij} \det \wt{A_{ij}}
\]

For example,

\[
  \det \begin{pmatrix}
    0 & 1 & 0 & 2\\
    3 & 0 & 1 & 0\\
    1 & 0 & 2 & 3\\
    -1 & 2 & 3 & 4

  \end{pmatrix} = -1 \* \det
  \begin{pmatrix}
    3 & 1 & 0\\
    1 & 2 & 3\\
    -1 & 3 &4
  \end{pmatrix}
  + 2 \det
  \begin{pmatrix}
    0 & 0 & 2\\
    3 & 1 & 0\\
    1 & 2 & 3
  \end{pmatrix}
\]

Other properties:
\begin{itemize}

\item \(\det A\) is zero if two rows are linearly dependent \(\leftrightarrow \rank A < n\)

\item if rows are interchanged, then \(\det\) changes sign

\item if a row is multiplied by \(k\), then \(\det\) is scaled by \(k\)

\item if a multiple of a row \(i\) is added to row \(j\), then \(\det\) is unchanged

\item \(\det\) is linear along each row and column

\item \(\det AB = \det A \* \det B\)
\item \(\det \begin{pmatrix}
    A' & *\\
    0 & A''
  \end{pmatrix} = \det A' \det A''\) -- a simiar result holds for any number of \emph{blocks}
\item \(\det A^{t} = \det A\)
\item \(A\) is invertible \(\lra\) \(\det A \neq 0\)
\item If \(A\) is invertible, then \(\det A^{-1} = (\det A)^{-1}\)
\item If \(A, B\) are similar, then \(\det A = \det B\).
  \begin{note*}
    \(A, B\) are similar iff there exists an invertible \(Q\) such that \(A = Q^{-1} B Q\)
  \end{note*}
\item if A is invertible, then
  \[
    A^{-1} = \frac{1}{\det A}
    \begin{pmatrix}
      \det \wt{A_{11}} & -\det \wt{A_{21}} & \dots\\
      - \det \wt{A_{12}} & \ddots & \\
      \det \wt{A_{13}} & \dots
    \end{pmatrix}
  \]
\end{itemize}

\section{Diagonalization}
\label{sec:diag}
\subsection{Eigenvalues, Eigenvectors}
\label{subsec:eig}

Motivation: simplification of the matrix form, decomposition of automorphisms (eg computation of \(A^{100}\))

Recall that \(A\) is diagonal if \(A = \begin{pmatrix}
  A_{11} & 0 & 0 & \dots\\
  0 & A_{22} & 0 & \dots\\
  \vdots & &\ddots & \vdots\\
  0& \dots & &  A_{nn}
\end{pmatrix}\)

\begin{definition}
  For \(V\) a finite dimensional vector space, \(T: V\to V\) a linear
  transformation, \(T\) is \textbf{diagonalisable} if there exists an ordered
  basis \(\beta\) such that \([T]_{\beta}\) is diagonal.

  If \(A\in M_{n\times n}(F)\), then A is \textbf{diagonalisable} if
  \(L_{A}: F^{n} \to F^{n}\) is diagonalisable. Equivalently, \(A\) is
  similar to a diagonal matrix.
\end{definition}

If \(T\) is diagonalisable and \([T]_{\beta} = \begin{pmatrix}
  D_{11} & 0 & \dots \\
  & \ddots & \\
  0 & \dots & D_{nn}
\end{pmatrix}\), where \(\beta = (v_{1}, v_{2}, \dots, v_{n})\), then
\(Tv_{1} = D_{11}v_{1}, \dots,Tv_{n} =D_{nn}v_{n}\).

\begin{definition}
  \(Tv = \lambda v\) with \(v\neq 0, \lambda \in F\), then \(v\) is an
  \textbf{eigenvector} of \(T\) with corresponding \textbf{eigenvalue}
  \(\lambda\).

  Similarly, an eigenvalue of \(A\) is an eigenvalue of \(L_{A}\).
\end{definition}

\begin{example}
  If \(T=\lambda I_{v}\) (ie \(T(v) = \lambda v\  \forall v \in V\)),
  then any nonzero \(v\in V\) is an eigenvector of \(T\).
\end{example}
\begin{example}
  If \(A = \begin{pmatrix}
    1 & 0 & 0\\
    0 & 2 & 0\\
    0 & 0 & 3
  \end{pmatrix}\), then \(e_{1}, e_{2},e_{3}\) are eigenvectors with
eigenvalues \(1, 2, 3\).
\end{example}

\begin{example}
  If \(T\) is arbitrary, then eigenvectors with the eigenvalue 0 are the nonzero elements of \(\ker(T)\).
\end{example}
\begin{example}
  If \(T: \RR^{2} \to \RR^{2}\) is a rotation by angle
  \(\alpha\in (0, \pi)\), then there are no eigenvectors \(\ra T\) is
  not diagonalisable.
\end{example}
\begin{example}
  If \(A = \begin{pmatrix}
    4 & 3\\
    -2 & -1
  \end{pmatrix}\), then \(v=\cv{1; -1}\) is an eigenvector with the eigenvalue 1.
\end{example}

From above, if \(T\) is diagonalisable, then \(V\) has a basis consisting of eigenvectors of \(T\).

Conversely, if \(\beta = (v_{1}, v_{2}, v_{3}, \dots)\) i sa basis of
eigenvectors
\(T(v_{1}) = \lambda v_{1}, \dots, T(v_{2}) = \lambda v_{2}\), then
\(T\) is diagonalisable.

\subsection{Finding Eigenvectors and Eigenvalues}
\label{subsec:eigf}

If \(T(v)= \lambda v, v\neq 0\), then
\(0 = T(v) - \lambda v = T(v) - \lambda I_{v}(v) = (T-\lambda
I_{v})(v) = 0 \lra v\in \ker(T-\lambda I_{v})\).

Thus, \(T\) has an eigenvalue \(\lambda \lra\)
\(\ker(T-\lambda I_{v}) \neq 0 \lra T-\lambda I_{v}\) is not injective
\(\lra T- \lambda I_{v}\) is not invertible.

In that case, the eigenvectors of an eigenvalue \(\lambda\) are the
non-zero elements of \(\ker(T-\lambda I_{v})\).

Similarly for \(A\in M_{n\times n}(F)\).

\begin{example}
  Let \(A = \begin{pmatrix}
    4 & 3\\
    -2 & -1
  \end{pmatrix}\), \(\lambda \in F\).

  Then
  \(\det(A-\lambda I) = (4 - \lambda)(-1-\lambda) + 6 =
  (\lambda-1)(\lambda - 2)\).

  Thus \(\lambda_{1} = 1, \lambda_{2} = 2\).

  Find the corresponding eigenvectors to obtain \(\cv{1, -1}\)
  corresponding to \(\lambda = 1\) and \(\cv{3, -2}\) corresponding to
  \(\lambda = 2\).

  Since they span \(F^{2}\), \(A\) is diagonalisable.
\end{example}

\begin{definition}
  The \textbf{characteristic polynomial} of \(A\) is the polynomial

  \begin{equation*}
    f(\lambda) = \det (A-\lambda I)
  \end{equation*}

\end{definition}
\begin{example}
  If \(A = \begin{pmatrix}
    a & b\\
    c & d
  \end{pmatrix}\), then \(f(\lambda) = \lambda^{2} -(a+d)\lambda + (ad - bc)\).
\end{example}

\begin{remark}
  If \(A, B\) are similar, they have the same characteristic
  polynomial. Hence if \(T\in \Hom(V,V)\), then the characteristic
  polynomial can be defined for \(T\) as the characteristic polynomial
  of \([T]_{\beta}\) for any \(\beta\).
\end{remark}

\begin{theorem}
  For \(A\in M_{n\times n}(F)\),

  \begin{enumerate}[label=\alph*)]
  \item The characteristic polynomial of \(A\) has degree \(n\), with the leading coefficient \((-1)^{n}\).
  \item The number of distinct eigenvalues is less than or equal to
    \(n\).
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{claim*}
    For \(B\in M_{n\times n}(F)\), with entries that are linear
    functions in \(\lambda\), then \(\det B\) is a polynomial in
    \(\lambda\) of degree at most \(n\).
  \end{claim*}
\begin{proof}
  If \(n=1\), the claim follows from the fact that \(\det (a) = a \)
  for any \(a \in F\).
  Suppose the claim is true for some \(n = k-1\in \NN\).

  Note that
  \(\det B = \sum_{j=1}^{n} (-1)^{i+j} B_{ij}\det \wt{B_{ij}}\ra
  \deg(\det B) \leq k\) by induction, since \(\deg(\det \wt{B_{ij}}) \leq k-1\) by inductive hypothesis.
\end{proof}
\begin{enumerate}[label=\alph*)]
\item

To prove (a), we use induction on n again.

The statement is true for \(n = 1\).

Suppose it is also true for some \(n-1\). Then


\begin{align*}
  f(\lambda) &= \det(A-\lambda I)\\
             &= (A_{11}-\lambda)\det \begin{pmatrix}
    a_{22} - \lambda & A_{23} & \dots\\
    \vdots & & \vdots\\
    A_{n2} & \dots & A_{nn} - \lambda
  \end{pmatrix} - A_{12} \det \begin{pmatrix}
    A_{21} & A_{23} & \dots\\
    \vdots & A_{33} - \lambda & \dots\\
    & \ddots &
  \end{pmatrix} \pm \dots
\end{align*}

Note that the determinants of the above expressions correspond to
\((n-1)\*(n-1)\) matrices, and thus by inductive hypothesis their
leading coefficient is \((-1)^{n-1}\). Expanding the first brackets,
we can see that the highest power of \(\lambda\) in the expression has
the coefficient \((-1)^{n}\), as required.

Thus, the claim is true by induction.

\item

  \begin{note*}
    See App E, Cor 2
    Ex 5.1/21
  \end{note*}

  \begin{claim*}
    Let \(T\in\LL(V)\). Suppose
    \(\lambda_{1}, \lambda_{2}, \dots, \lambda_{m}\) are distinct
    eigenvalues of \(T\) and \(v_{1}, v_{2}, \dots, v_{m}\) are
    corresponding eigenvectors. Then \(v_{1}, v_{2}, \dots, v_{m}\)
    are linearly independent.
  \end{claim*}

  \begin{proof}
    By way of contradiction, suppose that
    \(v_{1}, v_{2}, \dots, v_{m}\) are linearly dependent. Since they
    are linearly dependent, there exists \(k\in\ZZ\) such that
    \[v_{k}\in\spn(v_{1}, v_{2}, \dots, v_{k-1}).\]

    Thus there exist \(a_{1}, a_{2}, \dots, a_{k-1}\) such that

    \begin{equation}
      \label{eq:1}
      v_{k} = \sum_{i=1}^{k-1}a_{i}v_{i}.
    \end{equation}

    Applying \(T\) to both sides, we obtain

    \begin{equation*}
      T(v_{k}) = \lambda_{k}v_{k} =  \sum_{i=1}^{k-1}a_{i}\lambda_{i}v_{i}.
    \end{equation*}

    From~(\ref{eq:1}),

    \begin{equation*}
      0 =  \sum_{i=1}^{k-1}a_{i}(\lambda_{k} - \lambda_{i})v_{i}
    \end{equation*}

    Since all \(\lambda_{i}\) are distinct and all \(v_{i}\) are
    linearly independent, then \(a_{i}\) are all zero, and thus
    \(v_{k}\) is zero, which is a contradiction to the hypothesis that
    \(v_{k}\) is an eigenvector.

    In conclusion, the assumption that \(v_{1}, v_{2}, \dots, v_{m}\) are
    linearly dependent is false.
  \end{proof}

  The claim above implies that \(m\leq\dim V = n\), where \(m\) is the
  number of distinct eigenvalues.
\end{enumerate}
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
